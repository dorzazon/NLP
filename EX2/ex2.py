import re
import random
import math
from collections import Counter

class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns a model from a given text.
        It supports language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """
    # done
    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n = n
        self.chars = chars

    # done - with notations
    def build_model(self, text, n=3, chars=False):  # should be called build_model
        """populates a dictionary counting all ngrams in the specified text.

            Args:
                text (str): the text to construct the model from.
                chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.vocabulary = Counter(text.split())
        if chars:
            self.model = self.__char_ngram(text, n)
            self.n_1_counts = self.__char_ngram(text, n-1)
        else:
            self.model = self.__word_ngram(text, n)
            self.n_1_counts = self.__word_ngram(text, n-1)

    def __load_text(self, text_path):
        with open(text_path, "r", encoding="utf8") as file:
            return file.read()

    def __char_Ngram(self, text, n=3):
        ngrams = {}
        for i in range(len(text) - n):
            seq = text[i:i + n]
            if seq not in ngrams.keys():
                ngrams[seq] = 0
            ngrams[seq] += 1
        return ngrams

    def __word_ngram(self, text, n=3):
        ngrams = {}  # contains the count for each ngram
        words = text.split()  # split to words
        try:  # catches exception if line is smaller than n
            for i in range(len(words) - n + 1):
                seq = ' '.join(words[i:i+n])  # get the n-sequence
                if seq not in ngrams.keys():
                    ngrams[seq] = 0
                ngrams[seq] += 1
        except Exception:
            pass
        return ngrams

    #####################################
    # done
    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """
        return self.model

    def generate(self, context=None, n=20):  # each time generate is applied i get context - then calculate the count for
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        if context == None:
            context = random.choices(list(self.model.keys()), weights=list(self.model.values()))[0]  # rand context
        out = context  # add context to out string
        context_n_minus_1 = context.split()[-(self.n - 1):]  # slide the window to the n-1
        context_n_minus_1 = ' '.join(context_n_minus_1)  # combine back to string
        # generate n words
        for i in range(1, n):
            match_ngrams = {}
            for ngram in list(self.model.keys()):
                if ngram.find(context_n_minus_1) == 0:  # optional ngram
                    match_ngrams[ngram] = self.model[ngram]  # add ngram to optional dict
            gen_ngram = random.choices(list(match_ngrams.keys()), weights=list(match_ngrams.values()))[0]  # rand word
            gen_word = gen_ngram.split()[-1]
            out = out + ' ' + gen_word  # add next work to out string
            context_n_minus_1 = context_n_minus_1 + ' ' + gen_word  # add next work to context_n_minus_1 string
            context_n_minus_1 = context_n_minus_1.split()[-(self.n - 1):]  # slide the window to the n-1
            context_n_minus_1 = ' '.join(context_n_minus_1)  # combine back to string
        return out

    def evaluate(self, text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.
           explanation: 1. in the model init i calculated the ngram counts for n and n-1
                        2. in the evaluation i iterate over all ngrams in the sentence and calculate
                         the probability with the equation count(ngram) / count(ngram -1)
                        3. the returned probability is the sum of ln() probabilities

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        # construct the n -1 Ngram to find if smoothing is needed
        test_n_1_counts = self.__word_ngram(text, self.n - 1)
        smoothing = False
        # check if all n-1 Ngrams is in LM Ngrams
        if test_n_1_counts.keys() != self.n_1_counts.keys(): # todo: is in instead
            smoothing = True
        text = text.split()  # split text to words
        log_probability = 0
        for i in range(len(text) - self.n + 1):
            # construct the current ngram
            cur_ngram = text[i:i+self.n]
            cur_ngram = ' '.join(cur_ngram)
            # construct the current n -1 gram
            cur_n_1_gram = text[i:i + self.n - 1]
            cur_n_1_gram = ' '.join(cur_n_1_gram)
            if smoothing:
                log_probability += math.log(self.smooth(cur_ngram))
            else:
                log_probability += math.log(self.model[cur_ngram] / self.n_1_counts[cur_n_1_gram])
        return log_probability

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        ngram_split = ngram.split()
        cur_n_1_gram = ngram_split[:self.n - 1]  # get the n - 1 ngram
        cur_n_1_gram = ' '.join(cur_n_1_gram)
        return (self.model.get(ngram, 0) + 1) / (self.n_1_counts.get(cur_n_1_gram, 0) + len(self.n_1_counts))

# done
def normalize_text(text):
    """Returns a normalized string based on the specify string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decisions in the header of the function.
       my steps are: 1. remove alll urls since they are very unique and doesnt add value to sentence understanding
                    2. remove all punctuation because it can interrupt the meaning of a word and cancel the count of
                    the same work with diffrent punctuation
                    3. lower case all text to make it case insensitive
       Args:
           text (str): the text to normalize

       Returns:
           string. the normalized text.
    """
    #text_no_urls = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', text, flags=re.MULTILINE)
    #text_no_punctuation = re.sub(r'[^\w\s]', '', text_no_urls)
    text_add_whitespace = re.sub(r'(?=[.])', ' ', text)
    text_lower_cased = text_add_whitespace.lower()
    return text_lower_cased

class Spell_Checker:
    """The class implements a context sensitive spell checker. The corrections
        are done in the Noisy Channel framework, based on a language model and
        an error distribution model.
    """

    def __init__(self, lm=None):
        """Initializing a spell checker object with a language model as an
        instance  variable. The language model should suppport the evaluate()
        and the get_model() functions as defined in assignment #1.

        Args:
            lm: a language model object. Defaults to None
        """
        self.language_model = lm

    def build_model(self, text, n=3):
        """Returns a language model object built on the specified text. The language
            model should support evaluate() and the get_model() functions as defined
            in assignment #1.

            Args:
                text (str): the text to construct the model from.
                n (int): the order of the n-gram model (defaults to 3).

            Returns:
                A language model object
        """
        language_model = Ngram_Language_Model(n=n)
        text = self.__prepare_text(text, n)
        language_model.build_model(text=text, n=n)
        self.language_model = language_model
        self.one_char_counter = Counter(list(text))  # uni-gram counter
        self.two_chars_counter = Counter(two_chars_at_text_list(text))  # bi-gram counter
        return language_model

    def __prepare_text(self, text, n):
        lines = text.split('.')
        res = []
        for line in lines:
            res.append(pad_sentence(line + ' .', n))
        return ' '.join(res)

    def add_language_model(self, lm):
        """Adds the specified language model as an instance variable.
            (Replaces an older LM dictionary if set)

            Args:
                ls: a language model object
        """
        self.language_model = lm

    def learn_error_tables(self, errors_file):
        """Returns a nested dictionary {str:dict} where str is in:
            <'deletion', 'insertion', 'transposition', 'substitution'> and the
            inner dict {str: int} represents the confusion matrix of the
            specific errors, where str is a string of two characters matching the
            row and column "indexes" in the relevant confusion matrix and the int is the
            observed count of such an error (computed from the specified errors file).
            Examples of such string are 'xy', for deletion of a 'y'
            after an 'x', insertion of a 'y' after an 'x'  and substitution
            of 'x' (incorrect) by a 'y'; and example of a transposition is 'xy' indicates the characters that are transposed.


            Notes:
                1. Ultimately, one can use only 'deletion' and 'insertion' and have
                    'substitution' and 'transposition' derived. Again,  we use all
                    four types explicitly in order to keep things simple.
            Args:
                errors_file (str): full path to the errors file. File format, TSV:
                                    <error>    <correct>


    Returns:
        A dictionary of confusion "matrices" by error type (dict).
"""
        text = open(errors_file, "r").read()
        text = text.split('\n')
        error_list = []
        for t in text:
            error_list.append(t.split('\t'))
        good_corrections = {'insertion': [], 'transposition': [], 'substitution': [], 'deletion':[]}
        for word in error_list:
            # find optional corrections for misspelled word
            optional_corrections = self.__corrections_edits1(word[0])
            # find the corrections which match the corrected word
            good_corrections = self.__find_good_corrections(optional_corrections, word[1], good_corrections)
        error_table = {}
        for corr_type in good_corrections:
            error_table[corr_type] = dict(Counter(good_corrections[corr_type]))
        self.error_table = error_table
        return error_table

    def __find_good_corrections(self, optional_corrections, word, good_corrections):
        """
        this function finds out of all possible corrections, the word correction with matches the correction input

        :param optional_corrections: (dict) {deletion: {corrected word, xy}, ...})
        :param word: (str) the correct form of the misspelld word
        :param good_corrections: (dict) {deletion: [xy1, xy2, ...]} dictionary of the previous words
        :return: good_corrections
        """
        for corr_type in optional_corrections:
            for corr in optional_corrections[corr_type]:
                if corr[0] == word:  # if the corrections is match
                    good_corrections[corr_type].append(corr[1])
        return good_corrections

    def add_error_tables(self, error_tables):
        """ Adds the specified dictionary of error tables as an instance variable.
            (Replaces an older value dictionary if set)

            Args:
                error_tables (dict): a dictionary of error tables in the format
                returned by  learn_error_tables()
        """
        self.error_table = error_tables

    def evaluate(self, text):
        """Returns the log-likelihod of the specified text given the language
            model in use. Smoothing is applied on texts containing OOV words

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        return self.language_model.evaluate(text)

    def spell_check(self, text, alpha):
        """ Returns the most probable fix for the specified text. Use a simple
            noisy channel model as the number of tokens in the specified text is
            smaller than the length (n) of the language model.

            Args:
                text (str): the text to spell check.
                alpha (float): the probability of keeping a lexical word as is.

            Return:
                A modified string (or a copy of the original if no corrections are made.)
        """
        text = normalize_text(text)
        text = pad_sentence(text, self.language_model.n)
        words_from_text = text.split()
        optional_sentences = []
        for word in words_from_text[self.language_model.n - 1:-1]:  # ignore the padding words
            context = get_context(word, words_from_text, self.language_model.n)
            corrected_words_candidates = {}
            pxy = self.__pxw(word, alpha)  # dictionary of (corrected word, pxy)
            all_e1 = self.__pxw(word, alpha, in_voc=False)
            for corr_word in all_e1:
                pxy1 = all_e1[corr_word]
                pxy2 = self.__pxw(corr_word, alpha=0)  # find the probability of the seconed mistake
                for w in pxy2:  # multiply all pxy with pxy1
                    if w not in pxy.keys():
                        pxy[w] = pxy1 * pxy2[w]
                    else:
                        pxy[w] = pxy1 * pxy2[w] + pxy[w]
            for corr_word in pxy:
                candidate_sentence = context.replace(word, corr_word)
                pw = self.evaluate(candidate_sentence)
                corrected_words_candidates[corr_word] = math.log(pxy[corr_word]) if pxy[corr_word] != 0 else 0 + pw
            # find the word the maximize p(x|y)*p(w)
            most_probable_word = max(corrected_words_candidates, key=corrected_words_candidates.get)
            word_idx = words_from_text.index(word)
            # append to list the full text with the corrected word
            optional_sentences.append(' '.join(words_from_text[:word_idx]
                                               + [most_probable_word] + words_from_text[word_idx + 1:]))
        # evaluate all optional sentences with 1 word correction
        optional_sentences_dict = {}
        for sent in optional_sentences:
            optional_sentences_dict[sent] = self.evaluate(sent)
        return max(optional_sentences_dict, key=optional_sentences_dict.get)

    def __pxw(self, word, alpha, in_voc=True):
        """
        calculates the conditional probability p(x|w)
        :param word: (str) the word associated too x
        :param alpha: (float) the probability for good spelling of text - derived from spell_check alpha
        :param in_voc: (boolean) indicates if the corrected words must exist in the language model vocabulary.
         this helps to finds second order corrections
        :return: (dict) {corrected word: p(x|w)}
        """
        # find candidates for the word
        if in_voc:
            candidates_dict = self.__in_voc_corrections(word)
        else:
            candidates_dict = self.__corrections_edits1(word)
        # calculate p(x|w)
        candid_pxy = {}
        # try each correction type
        for correction_type in candidates_dict:
            candidates_arr = candidates_dict[correction_type]
            if candidates_arr:
                for candidate in candidates_arr:  # candidate = (corrected word, xy)
                    try:
                        if candidate[0] != word:  # if the corrected word is different than the word
                            if correction_type == 'insertion':
                                candid_pxy[candidate[0]] = self.error_table[correction_type][candidate[1]] \
                                                           / self.one_char_counter[candidate[1][1]]  # ins(x,y) / count(y)
                            elif correction_type == 'deletion':
                                candid_pxy[candidate[0]] = self.error_table[correction_type][candidate[1]] \
                                                           / self.two_chars_counter[candidate[1]]  # del(x,y) / count(x,y)
                            elif correction_type == 'substitution':
                                candid_pxy[candidate[0]] = self.error_table[correction_type][candidate[1]] \
                                                           / self.one_char_counter[candidate[1][1]]  # sub(x,y) / count(y)
                            else:  # transposition
                                candid_pxy[candidate[0]] = self.error_table[correction_type][candidate[1]] \
                                                           / self.two_chars_counter[candidate[1]]  # ins(x,y) / count(x,y)
                    except Exception:
                        pass
        # normalize probabilities to sum to 1 - alpha
        norm_factor = sum(candid_pxy.values()) / (1 - alpha)
        for item in candid_pxy:
            if norm_factor != 0:
                candid_pxy[item] = candid_pxy[item] / norm_factor
        if alpha != 0:
            candid_pxy[word] = alpha
        return candid_pxy

    def __corrections_edits1(self, word):
        """
        All edits that are one edit away from `word`
        :param word: (str) the word to find corrections from
        :return: (dict) {correction_type: [(corrected word, xy)]}
        """
        letters = 'abcdefghijklmnopqrstuvwxyz '
        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        # (corrected word, key)
        corrections = {}
        corrections['insertion'] = [(L + R[1:], L[-1] + R[0]) for L, R in splits[1:] if R]  # x typed as xy
        corrections['transposition'] = [(L + R[1] + R[0] + R[2:], R[0] + R[1]) for L, R in splits if
                                     len(R) > 1]  # xy typed as yx
        corrections['substitution'] = [(L + c + R[1:], R[0] + c) for L, R in splits if R for c in
                                        letters]  # x typed as y
        corrections['deletion'] = [(L + c + R, L[-1] + c) for L, R in splits[1:] for c in letters]  # xy typed as x
        return corrections

    def __in_voc_corrections(self, word):
        """
        find corrections which is in the language model vocabulary
        :param word: (str) word to find correction from
        :returns : (dict) {correction_type: [(corrected word, xy)]}
        """
        corrections_1 = self.__corrections_edits1(word)
        in_voc_corrections = {}
        for arr in corrections_1:
            in_voc_corrections[arr] = [w for w in corrections_1[arr] if w[0] in self.language_model.vocabulary]
        return in_voc_corrections

def who_am_i():  # this is not a class method
    """Returns a dictionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'Dor Zazon', 'id': '312237803', 'email': 'zazond@post.bgu.ac.il'}

def two_chars_at_text_list(text):
    """
    this function creates list of all adjacent two chars in the text. this function helps calculate the chars bi-gram
    :param text: (str) the given text
    :returns : (list) list of all adjacent two chars in the text
    """
    l = []
    for i in range(len(text)):
        l.append(text[i:i+2])
    return l

def get_context(word, words_from_text, n):
    """
    get the context of "word" in the sentence + his dependencies
    :param word: the word to get its context (str)
    :param words_from_text: list of words in the text (list(str))
    :param n: the n of the Ngram model
    :return: sentence contains n-1 words before "word" to n-1 words after "word" (str)
    """
    # find word index in the sentence
    word_idx = words_from_text.index(word)
    # get the minimal sentece include "word" in the context according to n
    minimal_sentence = words_from_text[word_idx - (n-1): word_idx + n]
    # return sentence
    return ' '.join(minimal_sentence)

def pad_sentence(text, n):
    """
    pad the sentence with n-1 <s> at the start. i assume each sentence ends with ( .) instead of </s>
    :param text: text file (str)
    :param n: the n of the Ngram model
    :return: padded sentence
    """
    # here i assume every sentence finish with . as final "word" so i only insert n -1 <s> at the start
    pad = '<s> '*(n-1)
    return pad + text

