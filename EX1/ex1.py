import re
import sys
import math
from collections import Counter
import random


class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns a model from a given text.
        It supports language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """
    # done
    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n = n
        self.chars = chars

    # done - with notations
    def build_model(self, text, n=3, chars=False):  # should be called build_model
        """populates a dictionary counting all ngrams in the specified text.

            Args:
                text (str): the text to construct the model from.
                chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.vocabulary = Counter(text.split())
        if chars:
            self.model = self.__char_ngram(text, n)
            self.n_1_counts = self.__char_ngram(text, n-1)
        else:
            self.model = self.__word_ngram(text, n)
            self.n_1_counts = self.__word_ngram(text, n-1)

    def __load_text(self, text_path):
        with open(text_path, "r", encoding="utf8") as file:
            return file.read()

    def char_Ngram(self, text, n=3):
        ngrams = {}
        for i in range(len(text) - n):
            seq = text[i:i + n]
            if seq not in ngrams.keys():
                ngrams[seq] = 0
            ngrams[seq] += 1
        return ngrams

    def __word_ngram(self, text, n=3):
        ngrams = {}  # contains the count for each ngram
        words = text.split()  # split to words
        try:  # catches exception if line is smaller than n
            for i in range(len(words) - n + 1):
                seq = ' '.join(words[i:i+n])  # get the n-sequence
                if seq not in ngrams.keys():
                    ngrams[seq] = 0
                ngrams[seq] += 1
        except Exception:
            pass
        return ngrams

    #####################################
    # done
    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
        """
        return self.model

    def generate(self, context=None, n=20):  # each time generate is applied i get context - then calculate the count for
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """
        if context == None:
            context = random.choices(list(self.model.keys()), weights=list(self.model.values()))[0]  # rand context
        out = context  # add context to out string
        context_n_minus_1 = context.split()[-(self.n - 1):]  # slide the window to the n-1
        context_n_minus_1 = ' '.join(context_n_minus_1)  # combine back to string
        # generate n words
        for i in range(1, n):
            match_ngrams = {}
            for ngram in list(self.model.keys()):
                if ngram.find(context_n_minus_1) == 0:  # optional ngram
                    match_ngrams[ngram] = self.model[ngram]  # add ngram to optional dict
            gen_ngram = random.choices(list(match_ngrams.keys()), weights=list(match_ngrams.values()))[0]  # rand word
            gen_word = gen_ngram.split()[-1]
            out = out + ' ' + gen_word  # add next work to out string
            context_n_minus_1 = context_n_minus_1 + ' ' + gen_word  # add next work to context_n_minus_1 string
            context_n_minus_1 = context_n_minus_1.split()[-(self.n - 1):]  # slide the window to the n-1
            context_n_minus_1 = ' '.join(context_n_minus_1)  # combine back to string
        return out

    def evaluate(self, text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.
           explanation: 1. in the model init i calculated the ngram counts for n and n-1
                        2. in the evaluation i iterate over all ngrams in the sentence and calculate
                         the probability with the equation count(ngram) / count(ngram -1)
                        3. the returned probability is the sum of ln() probabilities

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        # construct the n -1 Ngram to find if smoothing is needed
        test_n_1_counts = self.__word_ngram(text, self.n - 1)
        smoothing = False
        # check if all n-1 Ngrams is in LM Ngrams
        if test_n_1_counts.keys() != self.n_1_counts.keys():
            smoothing = True
        text = text.split()  # split text to words
        log_probability = 0
        for i in range(len(text) - self.n + 1):
            # construct the current ngram
            cur_ngram = text[i:i+self.n]
            cur_ngram = ' '.join(cur_ngram)
            # construct the current n -1 gram
            cur_n_1_gram = text[i:i + self.n - 1]
            cur_n_1_gram = ' '.join(cur_n_1_gram)
            if smoothing:
                log_probability += math.log(self.smooth(cur_ngram))
            else:
                log_probability += math.log(self.model[cur_ngram] / self.n_1_counts[cur_n_1_gram])
        return log_probability

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """
        ngram_split = ngram.split()
        cur_n_1_gram = ngram_split[:self.n - 1]  # get the n - 1 ngram
        cur_n_1_gram = ' '.join(cur_n_1_gram)
        return (self.model[ngram] + 1) / (self.n_1_counts[cur_n_1_gram] + len(self.n_1_counts))

# done
def normalize_text(text):
    """Returns a normalized string based on the specify string.
       You can add default parameters as you like (they should have default values!)
       You should explain your decisions in the header of the function.
       my steps are: 1. remove alll urls since they are very unique and doesnt add value to sentence understanding
                    2. remove all punctuation because it can interrupt the meaning of a word and cancel the count of
                    the same work with diffrent punctuation
                    3. lower case all text to make it case insensitive
       Args:
           text (str): the text to normalize

       Returns:
           string. the normalized text.
    """
    #text_no_urls = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', text, flags=re.MULTILINE)
    #text_no_punctuation = re.sub(r'[^\w\s]', '', text_no_urls)
    text_add_whitespace = re.sub(r'(?=[.])', ' ', text)
    text_lower_cased = text_add_whitespace.lower()
    return text_lower_cased
# done
def who_am_i():
    """Returns a ductionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'Dor Zazon', 'id': '312237803', 'email': 'zazond@post.bgu.ac.il'}